# Example input.yaml for training SevenNet.
# '*' signifies default. You can check log.sevenn for defaults.

model:
    chemical_species: 'auto'
    cutoff: 6.0
    channel: 128
    is_parity: False
    lmax: 3
    num_convolution_layer: 5
    irreps_manual:
        - "128x0e"
        - "128x0e+64x1e+32x2e+32x3e"
        - "128x0e+64x1e+32x2e+32x3e"
        - "128x0e+64x1e+32x2e+32x3e"
        - "128x0e+64x1e+32x2e+32x3e"
        - "128x0e"

    weight_nn_hidden_neurons: [64, 64]
    radial_basis:
        radial_basis_name: 'bessel'
        bessel_basis_num: 8
    cutoff_function:
        cutoff_function_name: 'XPLOR'
        cutoff_on: 5.5

    act_gate: {'e': 'silu', 'o': 'tanh'}
    act_scalar: {'e': 'silu', 'o': 'tanh'}

    train_shift_scale: False
    train_denominator: False
    self_connection_type: 'linear'
train:
    random_seed: 1
    is_train_stress: True                         # Includes stress in the loss function
    epoch: 4                                    # Ends training after this number of epochs

    loss:                                 # Default is 'mse' (mean squared error)
        - ['Energy', 'MAE']
        - ['Force', 'L2MAE']
        - ['Stress', 'MAE']

    # Each optimizer and scheduler have different available parameters.
    # You can refer to sevenn/train/optim.py for supporting optimizer & schedulers
    optimizer: 'adamw'                             # Options available are 'sgd', 'adagrad', 'adam', 'adamw', 'radam'
    optim_param:
        lr: 0.01
        weight_decay: 0.001
    scheduler: 'onecyclelr'                    # 'steplr', 'multisteplr', 'exponentiallr', 'cosineannealinglr', 'reducelronplateau', 'linearlr'
    scheduler_param:
        max_lr: 0.01
        epochs: 4
        pct_start: 0.0025
        anneal_strategy: 'cos'
        div_factor: 25.0
        final_div_factor: 10000.0
    force_loss_weight : 1.00
    stress_loss_weight: 0.01
    per_epoch: 1                                 # Generate checkpoints every this epoch
    print_interval_steps: 10000
    epoch_mode: False 

    # ['target y', 'metric']
    # Target y: TotalEnergy, Energy, Force, Stress, Stress_GPa, TotalLoss
    # Metric  : RMSE, MAE, or Loss
    error_record:
        - ['Energy', 'MAE']
        - ['Energy', 'RMSE']
        - ['Force', 'MAE']
        - ['Force', 'RMSE']
        - ['Force', 'L2MAE']
        - ['Force', 'L2RMSE']
        - ['Stress', 'MAE']
        - ['Stress', 'RMSE']
        - ['Energy', 'Loss']
        - ['Force', 'Loss']
        - ['Stress', 'Loss']
        - ['TotalLoss', 'None']

    # Continue training model from given checkpoint, or pre-trained model checkpoint for fine-tuning
    #continue:
        #checkpoint: 'checkpoint_best.pth'         # Checkpoint of pre-trained model or a model want to continue training.
        #reset_optimizer: False                    # Set True for fine-tuning
        #reset_scheduler: False                    # Set True for fine-tuning

data:
    batch_size: 16                                 # Per GPU batch size.
    data_divide_ratio: 0.1                        # Split dataset into training and validation sets by this ratio

    shift: 'per_atom_energy_mean'                 # One of 'per_atom_energy_mean*', 'elemwise_reference_energies', float
    scale: 'force_rms'                            # One of 'force_rms*', 'per_atom_energy_std', float
    dataset_type: 'lmdb'

    # SevenNet automatically matches data format from its filename.
    # For those not `structure_list` or `.pt` files, assumes it is ASE readable
    # In this case, below arguments are directly passed to `ase.io.read`
    data_format_args:
        index: ':'                                # see `https://wiki.fysik.dtu.dk/ase/ase/io/io.html` for more valid arguments

    # validset is needed if you want '_best.pth' during training. If not, both validset and testset is optional.
    #load_trainset_path: ['./structure_list']  # Example of using ase as data_format, support multiple files and expansion(*)
    #load_trainset_path: ["/home/sr5/zetta/DB/OMat24/dataset/salex/train"]
    load_trainset_path: ["/home/sr5/zetta/DB/OMat24/dataset/train/aimd-from-PBE-1000-npt", "/home/sr5/zetta/DB/OMat24/dataset/train/aimd-from-PBE-1000-nvt", "/home/sr5/zetta/DB/OMat24/dataset/train/aimd-from-PBE-3000-npt", "/home/sr5/zetta/DB/OMat24/dataset/train/aimd-from-PBE-3000-nvt", "/home/sr5/zetta/DB/OMat24/dataset/train/rattled-1000", "/home/sr5/zetta/DB/OMat24/dataset/train/rattled-300", "/home/sr5/zetta/DB/OMat24/dataset/train/rattled-500", "/home/sr5/zetta/DB/OMat24/dataset/train/rattled-relax"]
    load_validset_path: ["/home/sr5/zetta/DB/OMat24/dataset/valid/aimd-from-PBE-1000-npt", "/home/sr5/zetta/DB/OMat24/dataset/valid/aimd-from-PBE-1000-nvt", "/home/sr5/zetta/DB/OMat24/dataset/valid/aimd-from-PBE-3000-npt", "/home/sr5/zetta/DB/OMat24/dataset/valid/aimd-from-PBE-3000-nvt", "/home/sr5/zetta/DB/OMat24/dataset/valid/rattled-1000", "/home/sr5/zetta/DB/OMat24/dataset/valid/rattled-300", "/home/sr5/zetta/DB/OMat24/dataset/valid/rattled-500", "/home/sr5/zetta/DB/OMat24/dataset/valid/rattled-relax"]
    #load_validset_path: ['./valid.extxyz']
    #load_testset_path:  ['./sevenn_data/mydata.pt']  # Graph can be preprocessed using `sevenn_graph_build` and accessible like this
